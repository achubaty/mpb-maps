---
title: "Mountain pine beetle outbreak maps"
author:
- name: "Alex M. Chubaty"
  affiliaton: |
    FOR-CAST Research & Analytics
    PO BOX 96026 West Springs
    Calgary, AB  T3H 0L3
    +1.403.708.5790
    achubaty@for-cast.ca
- name: "Eliot J. B. McIntire"
  affiliaton: |
    Canadian Forest Service
    Pacific Forestry Centre  
    506 Burnside Road W  
    Victoria, BC V8Z 1M5
    +1.250.298.2374
    eliot.mcintire@canada.ca
date: "January 11, 2018"
output:
  pdf_document:
    toc: yes
    toc_depth: 3
bibliography: bibliography.bib
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(cache = TRUE, cache.path = "cache/", echo = FALSE,
                      message = FALSE, warning = FALSE, fig.path = "figures/")

## attach packages
suppressPackageStartupMessages({
  library(Require)
  Require(c("data.table", "fasterize", "future", "magrittr", "parallel", "raster", "reproducible",
            "rgdal", "rgeos", "RPostgres", "sf", "sp", "stringr", "RColorBrewer"))
  Require("achubaty/amc@development")
})

renviron <- c(".Renviron", "../.Renviron")
if (any(file.exists(renviron))) {
  f <- renviron[which(file.exists(renviron))][1]
  readRenviron(f)

  gisDBconn <- DBI::dbConnect(drv = RPostgres::Postgres(),
                              host = Sys.getenv("PGHOST"),
                              port = Sys.getenv("PGPORT"),
                              dbname = Sys.getenv("PGDATABASE"),
                              user = Sys.getenv("PGUSER"),
                              password = Sys.getenv("PGPASSWORD"))
} else {
  gisDBconn <- NULL
}

## determine OS and whether we are on a CFS machine
._CFS_. <- grepl("W-VIC", Sys.info()[["nodename"]]) ## TODO: remove; obsolete
._OS_. <- Sys.info()[["sysname"]]
._USER_. <- Sys.info()[["user"]]

plan("multisession")

scratchDir <- ifelse(dir.exists("/mnt/scratch"), "/mnt/scratch/MPB", file.path(dirname(tempdir()), "scratch/MPB")) %>% 
  checkPath(., create = TRUE)

rasterOptions(default = TRUE)
rasterOptions(
  maxmem = 1e+12,
  tmpdir = scratchDir
)

## set work dirs based on computer used
if (isTRUE(._USER_. == "achubaty")) {
  if (._OS_. == "Linux") {
    maps.dir <- "~/data"
    work.dir <- "~/Documents/GitHub/MPB/mpb-maps"
  }
}
setwd(work.dir)
rdata.path <- file.path(maps.dir, "MPB", "Rmaps") %>% checkPath(create = TRUE)
fig.path <- file.path(work.dir, "figures") %>% checkPath(create = TRUE)

stopifnot(all(dir.exists(c(fig.path, maps.dir, rdata.path, work.dir))))

## additional options
._NUM_CPUS_. <- min(parallel::detectCores() / 2, 16) ## max cpus to use

# options below only need to be changed if you want to reprocess the data
# - e.g., you get new MPB map data
# - e.g., you want to change the resolution/extent/etc. of the maps
#        (before rerunning just to change res, can you `clip` out a subset?)
#
._MAPS_USERAW_. <- FALSE  ## read in raw maps from data
._MAPS_REPROJ_. <- FALSE  ## reproject raw maps to `boreal`
._MAPS_MKRSTR_. <- FALSE  ## convert the maps to rasters

._RES_MAPS_. <- 250.0     ## map resolution, in metres
._PRJ_MAPS_. <- paste("+proj=lcc +lat_1=49 +lat_2=77 +lat_0=0 +lon_0=-95",
                      "+x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs")
._CRS_MAPS_. <- structure(list(epsg = NA_integer_, proj4string = ._PRJ_MAPS_.), class = "crs")
## map projection to use (matches kNN data)
```

\newpage

## Overview of MPB map data

We use AB `SpatialPoints` data and BC `SpatialPolygons` data.
AB has intensive surveying strategy that covers the whole province, whereas BC uses an extensive surveying approach so polygons provide a better estimate for MPB red attack.
All data are reprojected to match the `boreal` projection and cropped to the study area (currently AB, BC, SK).
When rasterizing, care must be taken to ensure points and polygon data are represented in each pixel at the same scale.
Points data are summed for each pixel, whereas for polygons an average of 1125 stems/ha is used, weighted by the proportion of a pixel overlapped by the polygon.
See @Cooke:2017fem for more details.

### BC forest pest metadata

```{r bc-metadata, comment=""}
metadata <- file.path(maps.dir, "MPB", "bc_feds_pest_metatdata.TXT")
cat(readLines(metadata), sep = '\n')
```

### AB aerial surveys metadata

<!-- TODO: get AB metadata -->

```{r ab-metadata, comment=""}
#metadata <- file.path(maps.dir, "MPB", "bc_feds_pest_metatdata.TXT")
#cat(readLines(metadata), sep = '\n')
```

## Canadian administrative boundaries

```{r can_adm}
if (isTRUE(._MAPS_USERAW_.)) {
  adm.path <- file.path(maps.dir, "CAN_adm") %>% checkPath(create = TRUE)
  
  for (s in 0:3) {
    assign(paste0("CAN_adm", s),
           Cache(prepInputs, dlFun = "raster::getData", "GADM", country = "CAN",
                 level = s, path = adm.path, destinationPath = adm.path,
                 targetFile = paste0("gadm36_CAN_", s, "_sp.rds"), ## TODO: this will change as GADM data update
                 fun = "base::readRDS"),
           envir = .GlobalEnv)
    save(list = paste0("CAN_adm", s), file = file.path(adm.path, paste0("CAN_adm", s, ".Rdata")))
  }
}
```

## Canadian boreal forest maps

```{r import-maps-boreal}
if (isTRUE(._MAPS_USERAW_.)) {
  west <- CAN_adm1[CAN_adm1$NAME_1 %in% c("Alberta", "Saskatchewan"), ]
  west <- Cache(postProcess, west, targetCRS = ._PRJ_MAPS_., filename2 = NULL)
  
  files <- file.path(rdata.path, c("boreal.RData", "boreal.can.RData"))
  if (all(file.exists(files))) {
    lapply(files, load, envir = .GlobalEnv)
  } else {
    boreal <- Cache(prepInputs,
                    targetFile = "NABoreal.shp",
                    alsoExtract = "similar",
                    archive = asPath("boreal.zip"),
                    destinationPath = maps.dir,
                    url = "http://cfs.nrcan.gc.ca/common/boreal.zip",
                    fun = "sf::read_sf",
                    useSAcrs = TRUE,
                    studyArea = west,
                    filename2 = NULL,
                    userTags = c("stable", "NorthAmericanBoreal"))
    boreal.can <- boreal[boreal$COUNTRY == "CANADA", ]
    save(boreal, file = files[1])
    save(boreal.can, file = files[2])
  }
  rm(files)
  
  west.provs <- c("Alberta", "British Columbia", "Saskatchewan")
  canada1.west <- CAN_adm1[CAN_adm1$NAME_1 %in% west.provs, ]
  canada2.west <- CAN_adm2[CAN_adm2$NAME_1 %in% west.provs, ]

  west.boreal <- st_transform(st_as_sf(canada1.west), ._CRS_MAPS_.)
  west2.boreal <- st_transform(st_as_sf(canada2.west), ._CRS_MAPS_.)
  canada1.boreal <- st_transform(st_as_sf(CAN_adm1), ._CRS_MAPS_.)
  canada2.boreal <- st_transform(st_as_sf(CAN_adm2), ._CRS_MAPS_.)
  
  # boreal.west takes a long time, so put in another process
  boreal.west %<-% st_intersection(boreal, west.boreal) # %>% st_buffer(., 0)
  
  f <- normalizePath(file.path(rdata.path, "west_boreal.shp"))
  st_write(west.boreal, f, delete_dsn = TRUE)
  fout <- normalizePath(file.path(rdata.path, "west_boreal.tif"), mustWork = FALSE)
  west.boreal.raster <- fasterize(west.boreal, raster(west.boreal, res = ._RES_MAPS_.))
  writeRaster(west.boreal.raster, filename = fout, overwrite = TRUE)
  writeRaster(west.boreal.raster, filename = extension(fout, ".grd"), overwrite = TRUE)
  rm(f, fout)

  ### Save these new map objects for later use
  objects2save <- c("boreal", "boreal.can", #"boreal.west",
                    "canada1.west", "canada2.west",
                    "canada1.boreal", "canada2.boreal",
                    "west.boreal", "west2.boreal", "west.boreal.raster")
  saveObjects(objects2save, rdata.path)
  rm(list = objects2save)
  rm(objects2save)
  .gc()
}
```

## MPB Stand Susceptibility Index

```{r mpb_ssi}
dout <- file.path(maps.dir, "MPB", "ab_mpb_ssi") %>% checkPath(create = TRUE)
fout <- file.path(dout, "ab_mpb_ssi.shp")

if (isTRUE(._MAPS_USERAW_.) && (._OS_. == "Windows") &&
    (NROW(subset(ogrDrivers(), grepl("GDB", name))) > 0)) {
  f <- file.path(maps.dir, "MPB", "Mountain_Pine_Beetle_SSI.gdb")
  stopifnot(file.exists(f))
  
  ## List all feature classes in a file geodatabase
  fc_list <- ogrListLayers(f)
  print(fc_list)
  
  ssi <- st_read(dsn = f, layer = "STAND_SUSC_INDEX_MPB")
  
  write_sf(ssi, dsn = fout)
} else {
  ssi <- st_read(fout) ## TODO: use prepInputs using 7z file from gdrive
}

ssi_reproj <- st_transform(ssi, ._CRS_MAPS_.)
r_ssi <- fasterize(ssi_reproj, raster(west.boreal[west.boreal$NAME_1 == "Alberta"], res = ._RES_MAPS_.), "MPB_SSI")
writeRaster(r_ssi, file.path(maps.dir, "MPB", "ab_mpb_ssi.tif"))
```

## MPB red attack data

### AB `SpatialPoints` and `SpatialPolygons` data

#### Download all AB MPB data

```{r download-ab-mpb}
##  file originally downloaded from:
##    ftp://ftp.env.gov.ab.ca/pub/MPB/Data/MPB_AERIAL_SURVEY.gdb.7z
##
##  [ password protected archive, downloaded October 19, 2016                       ]
##  [ see email from Aaron McGill (Aaron.McGill@gov.ab.ca) forwarded by Barry Cooke ]
##
```

#### Standardize AB MPB data

```{r standardize-ab-data}
## GDAL can only open GDB files on Windows
if (isTRUE(._MAPS_USERAW_.)) {
  if (isTRUE((._OS_. == "Windows") && (NROW(subset(ogrDrivers(), grepl("GDB", name))) > 0))) {
    f <- file.path(maps.dir, "MPB", "MPB_AERIAL_SURVEY.gdb")
    stopifnot(file.exists(f))
    
    ## List all feature classes in a file geodatabase
    fc_list <- ogrListLayers(f)
    print(fc_list)
    
    ## each layer represents data for a single year (2 of the last 3 characters in the name)
    years <- str_sub(fc_list, 10, 12)
    years <- ifelse(years < 50, paste0(20, years), paste0(19, years))
    
    ## Read the feature class
    layers <- structure(vector("list", length(years)), names = years)
    i <- 1
    lapply(fc_list, function(x) {
      fc <- st_read(dsn = f, layer = x)
      layers[[i]] <<- fc
      i <<- i + 1
    })
    
    ## Determine the FC extent, projection, and attribute information
    lapply(layers, summary)
    
    ## Extract points and polygon data into separate lists
    id_pnts <- which(substring(names(layers), 5, 5) == "x")
    id_poly <- which(substring(names(layers), 5, 5) == "p")
    
    ab_pnts <- layers[id_pnts]
    ab_poly <- layers[id_poly]
    
    ## order by year
    ab_pnts <- ab_pnts[order(names(ab_pnts))]
    ab_poly <- ab_poly[order(names(ab_poly))]
    ab_poly$`2018p` <- NULL ## TODO: remove this empty polygon
    
    stopifnot(all(sapply(ab_pnts, function(x) is(x$Shape, class2 = "sfc_POINT"))))
    stopifnot(all(sapply(ab_poly, function(x) is(x$Shape, class2 = "sfc_MULTIPOLYGON"))))
    rm(layers)
  } else if (isFALSE(is.null(conn))) {
    ## TODO: needs to fully match up with above
    years <- 2001:2019
    
    ab_pnts <- ab_poly <- structure(vector("list", length(years)), names = years)
    i <- 1
    lapply(years, function(x) {
      pnts <- st_read(dsn = gisDBconn, layer = paste0("MPB_AB_", x, "_pnts"))
      #poly <- st_read(dsn = gisDBconn, layer = paste0("MPB_AB_", x, "_poly")) ## 2001 missing? 2018 empty?
      ab_pnts[[i]] <<- pnts
      #ab_poly[[i]] <<- poly
      i <<- i + 1
    })
  }
  
  ## save to shapefiles
  output.dir <- file.path(maps.dir, "MPB", "ab_mpb_2019") %>% checkPath(create = TRUE)
  
  lapply(ab_pnts, function(x) {
    year <- max(unique(c(x$BEETLE_YR, x$SURVYEAR, x$SURV_YEAR, x$survyear)), na.rm = TRUE)
    f <- file.path(output.dir, paste0("ab_mpb_", year, "spot.shp"))
    write_sf(x, dsn = f)
  })
  lapply(ab_poly, function(x) {
    year <- max(unique(c(x$SURVYEAR, x$Survyear)), na.rm = TRUE)
    f <- file.path(output.dir, paste0("ab_mpb_", year, "poly.shp"))
    write_sf(x, dsn = f)
  })
}
```

#### Import AB points data

**NOTE:** Until we resolve how to handle point/polygon overlap [issue #13](https://github.com/achubaty/MPB/issues/13), we are only using the AB points data.

```{r import-maps-points}
if (isTRUE(._MAPS_USERAW_.)) {
  loadObjects("west.boreal.raster", rdata.path)
  
  #output.dir <- file.path(maps.dir, "MPB", "ab_mpb_2011")
  #output.dir <- file.path(maps.dir, "MPB", "ab_mpb_2016")
  #output.dir <- file.path(maps.dir, "MPB", "ab_mpb_2018")
  output.dir <- file.path(maps.dir, "MPB", "ab_mpb_2019")
  ab.pnts.files <- dir(path = output.dir, pattern = "spot")
  ab.pnts.dir.shp <- unique(sapply(strsplit(ab.pnts.files, "\\."), function(x) x[[1]]))
  
  ## parallel processing of data
  cl <- parallel::makeForkCluster(._NUM_CPUS_.)
  
  ## TODO: implement caching
  ab.pnts.boreal.raster.stack <- parallel::parLapplyLB(
    cl,
    ab.pnts.dir.shp,
    function(shp, crs, maps.dir, output.dir, rdata.path, rtm) {
      yr <- strsplit(shp, "_")[[1]][3] %>% substr(1, 4)
      pnts <- st_read(file.path(output.dir, extension(shp, ".shp")))
      names(pnts) <- c(toupper(names(pnts)[-length(names(pnts))]), "geometry")
      
      pnts.boreal <- st_transform(pnts, crs) ## TODO: is this necessary?
      if (is.null(pnts.boreal$NUM_TREES)) {
        warning("AB points data: missing 'NUM_TREES' for year ", yr, ".")
      }
      
      ## rasterize (fasterize can't use points; gdal_rasterize isn't working either)
      fout <- file.path(rdata.path, paste0("MPB_AB_pnts_", yr, ".tif"))
      out <- rasterize(pnts.boreal, rtm,
                       field = as.numeric(pnts.boreal$NUM_TREES), fun = "sum",
                       filename = extension(fout, ".grd"), overwrite = TRUE)
      writeRaster(out, filename = fout, overwrite = TRUE)
      return(out)
    },
    crs = ._CRS_MAPS_., maps.dir = maps.dir, output.dir = output.dir,
    rdata.path = rdata.path, rtm = west.boreal.raster
  ) %>%
    set_names(substr(sapply(strsplit(ab.pnts.dir.shp, "_"), function(x) x[[3]]), 1, 4)) %>%
    stack(filename = file.path(rdata.path, "ab_pnts_boreal.grd"), overwrite = TRUE)

  parallel::stopCluster(cl)
  
  # save these new map objects for later use
  saveObjects("ab.pnts.boreal.raster.stack", rdata.path)
  writeRaster(ab.pnts.boreal.raster.stack, file.path(rdata.path, "MPB_AB_pnts_1998-2016.tif"))
  
  ## cleanup workspace
  rm(ab.pnts.dir.shp, ab.pnts.files, ab.pnts.boreal.raster.stack, west.boreal.raster)
  .gc()
  unlink(tmpDir(), recursive = TRUE)
}
```

### BC `SpatialPolygons` and `SpatialPoints` data

#### Download all BC MPB data

```{r download-bc-mpb}
bc.data.dir <- file.path(maps.dir, "MPB", "bc_mpb_2016", "raw_data")
years <- as.character(1999:2016)

if (!dir.exists(bc.data.dir)) {
  sapply(file.path(bc.data.dir, years), dir.create, recursive = TRUE)
}

url <- "https://www.for.gov.bc.ca/ftp/HFP/external/!publish/Aerial_Overview/"
exts <- c(".dbf", ".prj", ".shp", ".shx")
data1999 <- append(
  as.list(paste0(url, "1999/shape/prov_ibm/PROV_IBM_point_point", exts)),
  as.list(paste0(url, "1999/shape/prov_ibm/PROV_IBM_poly_poly", exts))
)
data2000 <- append(
  as.list(paste0(url, "2000/provincial_data/shape/2000%20IBM/prv_ibm.dbf", exts[-2])),
  as.list(paste0(url, "2000/provincial_data/shape/2000%20IBM/prv_spot", exts[-2]))
)
data_zip <- list(
  paste0(url, "2001/shape_files/fhdata2001.zip"),
  paste0(url, "2002/fhf-nov15-2002.zip"),
  paste0(url, "2003/fhdata_2003_20040223/2003_AOS_shapefiles.zip"),
  paste0(url, "2004/fhf_complete_dataset_20050218.zip"),
  paste0(url, "2005/final/fhfdata_2005.zip"),
  paste0(url, "2006/fhdata_2006_final.zip"),
  paste0(url, "2007/final_version/MPB_only.zip"),
  paste0(url, "2008/2008_BC_overview.zip"),
  paste0(url, "2009/replacement%20spatial%20and%20MDB%20files-20100111.zip"),
  paste0(url, "2010/fhdata%20final%2012162010.zip"),
  paste0(url, "2011/final_2011_aos_July30.zip"),
  paste0(url, "2012/FHF_Final_12132012.zip"),
  paste0(url, "2013/FHF_2013_Jan24.zip"),
  paste0(url, "2014/2014_FHF_Jan23.zip"),
  paste0(url, "2015/final_prov_data/FHF_spatial_Feb11.zip"),
  paste0(url, "2016/AOS_2016_Shapefiles_and_TSA_Spreadsheet_Jan25.zip") ## TODO: add 2017 onward
)

## the 1999 data are visible, but downloads fail (404)
lapply(data1999, function(x) {
  tryCatch(
    dl.data(x, dest = file.path(bc.data.dir, "1999")),
    error = function(e) warning(e)
  )
})

## the 2000 data are visible, but downloads fail (404)
lapply(data2000, function(x) {
  tryCatch(
    dl.data(x, dest = file.path(bc.data.dir, "2000")),
    error = function(e) warning(e)
  )
})

years <- years[-(1:2)] ## TEMPORARY: omit 1999 and 2000 due to missing data
for (i in 1:length(years)) {
  tryCatch(
    dl.data(data_zip[[i]], dest = file.path(bc.data.dir, years[i]), unzip = TRUE),
    error = function(e) warning(e)
  )
}
```

#### Standardize BC MPB data

The BC data include more than just MPB attack, and the filenames etc. are not standardized, so we need to do some initial processing to make these datasets more script-friendly.
We need to extract all the points/polygons that were damanged by mountian pine beetle (coded as `IBM`).

```{r standardize-bc-data}
if (isTRUE(._MAPS_USERAW_.)) {
  bc_pnts <- list(
  #  file.path(bc.data.dir, "1999", ""),
  #  file.path(bc.data.dir, "2000", ""),
    file.path(bc.data.dir, "2001", "PRV_SPOT.SHP"),
    file.path(bc.data.dir, "2002", "prv_shape.shp"),
    file.path(bc.data.dir, "2003", "SHAPE_FILES", "fhf_spot_2003.shp"),
    file.path(bc.data.dir, "2004", "fhf_spot_2004.shp"),
    file.path(bc.data.dir, "2005", "fhf_spot.shp"),
    file.path(bc.data.dir, "2006", "fhf_spot.shp"),
    file.path(bc.data.dir, "2007", "MPB_only", "2007_prov_fhf_spot_IBM_only_final.shp"),
    file.path(bc.data.dir, "2008", "2008_BC_overview_spot.shp"),
    file.path(bc.data.dir, "2009", "fhf_spot.shp"),
    file.path(bc.data.dir, "2010", "2010_fhf_spot.shp"),
    file.path(bc.data.dir, "2011", "fhf_spot_2011.shp"),
    file.path(bc.data.dir, "2012", "FHF_SPOT_2012.shp"),
    file.path(bc.data.dir, "2013", "FHF_2013_Spot.shp"),
    file.path(bc.data.dir, "2014", "FHF_SPOT_2014.shp"),
    file.path(bc.data.dir, "2015", "FHF_Spot_2015.shp"),
    file.path(bc.data.dir, "2016", "FHF_Spot_2016.shp")
  )
  
  bc_poly <- list(
  #  file.path(bc.data.dir, "1999", ""),
  #  file.path(bc.data.dir, "2000", ""),
    file.path(bc.data.dir, "2001", "PRV_POLY.SHP"),
    file.path(bc.data.dir, "2002", "prv_poly.shp"),
    file.path(bc.data.dir, "2003", "SHAPE_FILES", "fhf_poly_2003.shp"),
    file.path(bc.data.dir, "2004", "fhf_poly_2004.shp"),
    file.path(bc.data.dir, "2005", "fhf_poly.shp"),
    file.path(bc.data.dir, "2006", "fhf_poly.shp"),
    file.path(bc.data.dir, "2007", "MPB_only", "2007_prov_fhf_poly_IBM_only_final.shp"),
    file.path(bc.data.dir, "2008", "2008_BC_overview_polygon.shp"),
    file.path(bc.data.dir, "2009", "fhf_poly.shp"),
    file.path(bc.data.dir, "2010", "2010_fhf_poly.shp"),
    file.path(bc.data.dir, "2011", "fhf_poly_2011.shp"),
    file.path(bc.data.dir, "2012", "FHF_POLY_2012.shp"),
    file.path(bc.data.dir, "2013", "FHF_2013_Poly.shp"),
    file.path(bc.data.dir, "2014", "FHF_POLY_2014.shp"),
    file.path(bc.data.dir, "2015", "FHF_Poly_2015.shp"),
    file.path(bc.data.dir, "2016", "FHF_Poly_2016.shp")
  )
  
  output.dir <- file.path(maps.dir, "MPB", "bc_mpb_2016")
  if (!dir.exists(output.dir)) dir.create(output.dir)
  
  ## points data
  lapply(bc_pnts, function(f) {
    suppressWarnings({
      file.rename(extension(f, ".DBF"), extension(f, ".dbf"))
      file.rename(extension(f, ".PRJ"), extension(f, ".prj"))
      file.rename(extension(f, ".SHP"), extension(f, ".shp"))
      file.rename(extension(f, ".SHX"), extension(f, ".shx"))
    })
    tmp <- st_read(extension(f, ".shp"))
    year.id <- which(strsplit(f, "/")[[1]] == basename(output.dir)) + 2
    year <- strsplit(f, "/")[[1]][year.id]
    ids <- which(tmp$FHF == "IBM")
    
    if (!("FHF" %in% names(tmp))) {
      warning("YEAR ", year, ":\n", paste(names(tmp), collapse = " "))
    }
  
    write_sf(tmp[ids, ], dsn = file.path(output.dir, paste0("ibm_spot_", year, ".shp")))
  })
  
  ## polygons data
  lapply(bc_poly, function(f) {
    suppressWarnings({
      file.rename(extension(f, ".DBF"), extension(f, ".dbf"))
      file.rename(extension(f, ".PRJ"), extension(f, ".prj"))
      file.rename(extension(f, ".SHP"), extension(f, ".shp"))
      file.rename(extension(f, ".SHX"), extension(f, ".shx"))
    })
    tmp <- st_read(extension(f, ".shp"))
    year.id <- which(strsplit(f, "/")[[1]] == basename(output.dir)) + 2
    year <- strsplit(f, "/")[[1]][year.id]
    ids <- which(tmp$FHF == "IBM")
  
    if (!("FHF" %in% names(tmp))) {
      warning("YEAR ", year, ":\n", paste(names(tmp), collapse = " "))
    }
  
    write_sf(tmp[ids, ], dsn = file.path(output.dir, paste0("ibm_poly_", year, ".shp")))
  })
  
  ## ensure all shapefiles have projections
  checkProjections <- function(type) {
    if ((tolower(type) == "spot" || tolower(type) == "pnts")) {
      type <- "spot"
    } else if ((tolower(type) == "poly")) {
      type <- "poly"
    } else {
      stop("invalid type.")
    }
    prj <- list.files(output.dir, pattern = ".prj", full.names = TRUE) %>%
      grep(type, ., value = TRUE)
    years.prj <- sapply(basename(prj), substr, 10, 13) %>% unname()
    years.shp <- sapply(bc_pnts, function(f) {
      year.id <- which(strsplit(f, "/")[[1]] == basename(output.dir)) + 2
      strsplit(f, "/")[[1]][year.id]
    })
    ids <- which(!(years.shp %in% years.prj))
    missing.prj <- sapply(ids, function(id) {
      gsub("[0-9][0-9][0-9][0-9][.]prj$", paste0(years.shp[id], ".prj"), prj[id])
    })
  
    allPrjEqual <- suppressWarnings(sapply(prj, readLines)) %>%
      unique() %>%
      length() %>%
      `==`(1)
    if (isTRUE(allPrjEqual)) {
      lapply(missing.prj, function(f) {
        message("Creating missing projection file:\n  ", f)
        file.copy(prj[1], f)
      })
    } else {
      warning("Not all points shapefiles use same projection!")
    }
    
    return(invisible(NULL))
  }
  
  checkProjections("pnts")
  checkProjections("poly")
}
```

#### Import BC polygons data

**NOTE:** Until we resolve how to handle point/polygon overlap, we are only using the BC polygons data.

```{r import-maps-polygons}
if (isTRUE(._MAPS_USERAW_.)) {
  loadObjects("west.boreal.raster", rdata.path)
  
  #output.dir <- file.path(maps.dir, "MPB", "bc_mpb_2011")
  output.dir <- file.path(maps.dir, "MPB", "bc_mpb_2016")
  bc.poly.files <- dir(path = output.dir, pattern = "poly")
  bc.poly.dir.shp <- unique(sapply(strsplit(bc.poly.files, "\\."), function(x) x[[1]]))

  ## in parallel: load, reproject, and rasterize each of the BC polygons
  #  do as much as possible in a single step (don't save intermediate objects) to minimize RAM use.
  #  total RAM used can be controlled by adjusting the number of CPUs used for processing.
  cl <- parallel::makeForkCluster(._NUM_CPUS_.)
  bc.poly.boreal.raster.stack <- parallel::parLapplyLB(
    cl,
    bc.poly.dir.shp,
    function(shp, crs, res, maps.dir, output.dir, rdata.path, rtm) {
      poly.boreal <- getOGR(shp, output.dir) %>% 
        ## reproject to boreal projection
        spTransform(., CRS(._CRS_MAPS_.$proj4string)) %>%
        fixErrors(objectName = "poly.boreal")
    
      stopifnot(gIsValid(poly.boreal))
    
      ## rasterize
      #  we use 1125 trees/ha, per Whitehead & Russo (2005), Cooke & Carroll (2017)
      #  we assign values to the raster based on the proportion of pixel area covered by the polygon
      #
      # TODO: use fasterize (requires sf) -- NOTE: can't do this yet as fasterize can't do 'getCover'
      #
      yr <- strsplit(shp, "_")[[1]][3]
      out <- rasterize(poly.boreal, west.boreal.raster, getCover = TRUE,
                       filename = file.path(rdata.path, paste0("MPB_BC_poly_", yr, ".grd")),
                       overwrite = TRUE)
      
      out <- out / 100 * 1125 * (._RES_MAPS_. / 100) ^ 2
      f <- filename(out) ## keep track of this tmp file for deletion later
      
      out <- writeRaster(out, filename = file.path(rdata.path, paste0("MPB_BC_poly_", yr, ".grd")),
                         overwrite = TRUE)
      writeRaster(out, filename = file.path(rdata.path, paste0("MPB_BC_poly_", yr, ".tif")),
                  overwrite = TRUE)
      
      try(unlink(f))
      return(out)
    },
    crs = ._CRS_MAPS_., res = ._RES_MAPS_., maps.dir = maps.dir, output.dir = output.dir,
    rdata.path = rdata.path,  rtm = west.boreal.raster
  ) %>% 
    set_names(sapply(strsplit(bc.poly.dir.shp, "_"), function(x) x[[3]])) %>%
    stack(filename = file.path(rdata.path, "bc_poly_boreal.grd"), overwrite = TRUE)

  parallel::stopCluster(cl)
  
  # save these new map objects for later use 
  saveObjects("bc.poly.boreal.raster.stack", rdata.path)
  writeRaster(bc.poly.boreal.raster.stack, file.name(rdata.path, "MPB_BC_poly_1997-2016.tif"))
  
  # clean up workspace 
  rm(bc.poly.dir.shp, bc.poly.files) 
  .gc()
  unlink(tmpDir(), recursive = TRUE)
}
```

### Combine AB and BC MPB maps

```{r combine-maps-all}
if (isTRUE(._MAPS_MKRSTR_.)) {
  ## TODO: verify that the resolution of rasters is 250 x 250
  
  ### combine bcab points and poly rasters from .tif files
  files <- dir(rdata.path, pattern = "[pnts|poly]_[0-9]{4}[.]tif", full.names = TRUE) %>% 
    grep("new", ., invert = TRUE, value = TRUE)
  
  yrs <- substr(basename(files), 13, 16)
  
  sfInit(cpus = ._NUM_CPUS_., parallel = TRUE) 
    sfLibrary(magrittr)
    sfLibrary(raster)
    sfExport("files", "rdata.path", "yrs")
    
    bcab.boreal.raster.stack <- sfClusterApplyLB(sort(unique(yrs)), function(y) {
      ids <- which(yrs == y)
      stopifnot(length(ids) %in% c(1, 2))
      
      f <- file.path(rdata.path, paste0("MPB_BCAB_", y, ".tif"))
  
      m <- if (length(ids) == 2) {
        r1 <- raster(files[ids[1]])
        r2 <- raster(files[ids[2]])
        mosaic(r1, r2, fun = sum, filename = f, overwrite = TRUE)
      } else if (length(ids == 1)) {
        writeRaster(raster(files[ids[1]]), filename = f, overwrite = TRUE)
      }
      
      return(m)
    }) %>%
      setNames(sort(unique(yrs))) %>%
      stack() %>% 
      writeRaster(filename = file.path(rdata.path, "mpb_bcab_boreal_1998-2016.grd"),
                  overwrite = TRUE)
  sfStop()    
  
  writeRaster(bcab.boreal.raster.stack,
              filename = file.path(rdata.path, "mpb_bcab_boreal_1998-2016.tif"),
              overwrite = TRUE) ## TODO: add options to compress raster depending on size

  ## also create a brick version
  bcab.boreal.raster.brick <- brick(bcab.boreal.raster.stack,
                                    filename = file.path(rdata.path, "bcab.boreal.raster.brick.grd"),
                                    overwrite = TRUE) %>%
    setNames(sort(unique(yrs)))

  writeRaster(bcab.boreal.raster.brick,
              filename = file.path(rdata.path, "mpb_bcab_boreal_1997-2016_brick.tif"),
              overwrite = TRUE) ## TODO: add options to compress raster depending on size

  saveObjects(c("bcab.boreal.raster.brick", "bcab.boreal.raster.stack"), rdata.path)

  ## cleanup workspace
  rm(bcab.boreal.raster.brick, bcab.boreal.raster.stack)
  .gc()
  unlink(tmpDir(), recursive = TRUE)
}
```

## Plotting maps

### Boreal forest maps

**The Canadian boreal forest:**

```{r plot-canada.boreal, fig.height=12, fig.width=16}
loadObjects(c("boreal.can", "canada1.boreal"), rdata.path)
darkgreen <- col2rgb("darkgreen") / 255
col.bor <- rgb(darkgreen[1], darkgreen[2], darkgreen[3], alpha = 0.5)
rm(darkgreen)

plot(canada1.boreal)
plot(boreal.can, col = col.bor, add = TRUE)

png(file.path(fig.path, "boreal.can.png"), type = "cairo", width = 1600, height = 1200)
plot(canada1.boreal)
plot(boreal.can, col = col.bor, add = TRUE)
dev.off()

rm(boreal.can)
```

**The western Canadian boreal forest:**

```{r plot-western.boreal, fig.height=12, fig.width=16}
loadObjects(c("boreal.west", "west.boreal"), rdata.path)
darkgreen <- col2rgb("darkgreen") / 255
col.bor <- rgb(darkgreen[1], darkgreen[2], darkgreen[3], alpha = 0.5)
rm(darkgreen)

plot(west.boreal)
plot(boreal.west, col = col.bor, add = TRUE)

png(file.path(fig.path, "boreal.west.png"), type = "cairo", width = 1600, height = 1200)
plot(west.boreal)
plot(boreal.west, col = col.bor, add = TRUE)
dev.off()

rm(boreal.west, west.boreal)
```

### MPB in western Canada

#### MPB `RasterStack`

```{r plot-mpb.western.boreal.raster.stack, fig.height=12, fig.width=12}
loadObjects(c("bcab.boreal.raster.stack", "west.boreal"), rdata.path)
colours <- brewer.pal(n = 9, name = "YlOrRd")
years <- substr(names(bcab.boreal.raster.stack), 2, 5)
last9 <- (length(years) - 8):length(years) # the last 9 years
subset <- last9[last9 > 0]

plot(bcab.boreal.raster.stack, subset, main = years, legend = FALSE, axes = FALSE,
     col = c("white", colours), addfun = function(x) plot(west.boreal, add = TRUE))
#legend("topright", legend = years[subset], fill = colours)

png(file.path(fig.path, "bcab.boreal.raster.stack.png"), type = "cairo",
    width = 2400, height = 1600)
plot(bcab.boreal.raster.stack, subset, main = years, cex.main = 3,
     legend = FALSE, axes = FALSE, col = c("white", colours),
     addfun = function(x) plot(west.boreal, add = TRUE))
#legend("topright", legend = years[subset], fill = colours)
dev.off()

rm(bcab.boreal.raster.stack, west.boreal)
```

#### MPB `RasterBrick` timeseries

```{r plot-mpb.western.boreal.time.series, fig.height=12, fig.width=12}
loadObjects(c("bcab.boreal.raster.brick", "west.boreal"), rdata.path)

# `spsample` needs planar coordinates, so need to reproject to latlong
crs.latlon <- CRS("+proj=longlat +datum=WGS84")
brick.latlon <- projectRaster(bcab.boreal.raster.brick, crs = crs.latlon)

# replace NAs with zeros (WHY??)
brick.latlon <- brick(lapply(1:nlayers(brick.latlon), function(x) {
  brick.latlon[[x]][is.na(brick.latlon[[x]])] <- 0
  return(brick.latlon[[x]])
}))
brick.latlon@title <- "MPB intensity"
brick.latlon <- writeRaster(brick.latlon, filename = file.path(rdata.path, "mpb_brick_latlon.grd"), overwrite = TRUE)

#brick.latlon <- brick(file.path(rdata.path, "mpb_brick_latlon.grd"))
years <- as.numeric(substr(names(bcab.boreal.raster.brick), 2, 5))
t.strt <- as.POSIXct(as.Date(paste(years, "-10-01", sep = "")))     ## why Oct 1?
t.stop <- as.POSIXct(as.Date(paste(years + 1, "-10-01", sep = ""))) ## why Oct 1?

# random sample of spatial points
sps <- spsample(west.boreal, 1, type = "random") %>% 
  spTransform(crs.latlon) %>%
  SpatialPointsDataFrame(data = data.frame(dat = 1))

# colour palette
colours <- brewer.pal(n = 9, name = "YlOrRd")

# `plotKML` timeseries
ts.kml <- new("RasterBrickTimeSeries", variable = "X", sampled = sps, rasters = brick.latlon,
              TimeSpan.begin = t.strt, TimeSpan.end = t.stop)
dims <- dim(brick.latlon)
#plotKML(ts.kml, colour_scale = c(rep("black",2), heat.colors(12)[12:1]),
#        pngwidth = dims[1], pngheight = dims[2], pngpointsize = 14)

# `rts` timeseries
ts.rts <- rts(brick.latlon, time = as.POSIXct(as.Date(t.strt)))
plot(ts.rts, col = c("white", colours)) # this is a *garbage* way to plot a TS

saveObjects(c("brick.latlon", "ts.kml", "ts.rts"), rdata.path)
rm(bcab.boreal.raster.brick, brick.latlon, ts.kml, ts.rts, west.boreal)
.gc()
```

```{r cleanup}
unlink(tmpDir(), recursive = TRUE)
```
